{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 8 - PART 2 LINEAR DATA FITTING WITH UNCERTANTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"HW8_2data.dat\", names=True, dtype=None)\n",
    "\n",
    "# print out all columns we just got for free\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function the normal way\n",
    "def linear(x, m, b):\n",
    "    return m*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's see what this data looks like\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black')\n",
    "\n",
    "# Cool! The data actually looks pretty linear so we can assume the function we are fitting *is* linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = opt.curve_fit(linear, data['X'], data['Y'])\n",
    "print(\"raw: \", *popt)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black')\n",
    "ax.plot(data['X'],linear(data['X'], *popt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = data['Y']-linear(data['X'],*popt)\n",
    "x = np.linspace(5,20,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],residual,s=15,c='black')\n",
    "ax.plot(x,x*0.0,color='purple')\n",
    "ax.set_xlim(6,16)\n",
    "\n",
    "outlier = np.absolute(residual)>0.2\n",
    "ax.scatter(data['X'][outlier],residual[outlier],s=60,edgecolor='cyan',facecolor='none')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back what we found last time\n",
    "# remember our values for the raw data were:\n",
    "# slope:    -0.058187\n",
    "# intercept: 0.565351\n",
    "popt, pcov = opt.curve_fit(linear, data['X'], data['Y'])\n",
    "print(\"raw: \", *popt)\n",
    "\n",
    "# we can use curve_fit to de-weight outliers automatically\n",
    "# remember our values after we removed the outliers were:\n",
    "# slope:    -0.043537\n",
    "# intercept: 0.395084\n",
    "\n",
    "# note, result is dependent on choice of loss function\n",
    "# need to see method to trf in order to use least_squares instead of leastsq\n",
    "popt_clean, pcov = opt.curve_fit(linear, data['X'], data['Y'], method='trf', \n",
    "                                 loss='arctan', f_scale=0.20)\n",
    "print(\"outliers accounted for: \", *popt_clean)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black',zorder=2)\n",
    "# cleaner error bars than last time, play around with the parameters\n",
    "ax.errorbar(data['X'],data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'],linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_clean), label='better')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets account for the errors\n",
    "# by pasing sigma, scipy knows to weight data according to their errors \n",
    "# naturally, sigma is expected to be the same size and correspond to y\n",
    "# notice: this only accounts for x error\n",
    "popt_err, pcov = opt.curve_fit(linear, data['X'], data['Y'], sigma=data['Y_ERR'])\n",
    "print(*popt_err)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'],data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'],data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'],linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'],linear(data['X'], *popt_err), label='errors')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does this accurately explore the parameter space though?\n",
    "# and what is the uncertainty on our data?\n",
    "# what if we had x error bars?\n",
    "\n",
    "# we'll explore a method utilizing the common \"monte carlo\" approach\n",
    "def mcFit(x, y, y_err):\n",
    "    slopes = list()\n",
    "    y_ints = list()\n",
    "    iters = 500 \n",
    "    for i in range(iters):\n",
    "        # remember random normal\n",
    "        weights = np.random.randn(len(y))\n",
    "#         weights2 = np.random.randn(len(data['R_GC']))\n",
    "\n",
    "        y_adj = y + y_err*weights\n",
    "        x_adj = x  # + data['x_err']*weights2\n",
    "\n",
    "        params, other = opt.curve_fit(linear, x_adj, y_adj)\n",
    "        slopes.append(params[0])\n",
    "        y_ints.append(params[1])\n",
    "    \n",
    "    return slopes, y_ints\n",
    "\n",
    "slope, intercept = mcFit(data['X'], data['Y'], data['Y_ERR'])\n",
    "\n",
    "print('mean slope: {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.mean(intercept)))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'], data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'], data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'], linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_err), label='errors')\n",
    "ax.plot(data['X'], linear(data['X'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh right, that outlier.\n",
    "\n",
    "mask = data['Y'] < 0.3\n",
    "\n",
    "xdat = data['X'][mask]\n",
    "ydat = data['Y'][mask]\n",
    "ydat_err = data['Y_ERR'][mask]\n",
    "\n",
    "print(\"data size: \", len(xdat))\n",
    "\n",
    "slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "\n",
    "print('slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}'.format(np.mean(slope), np.std(slope),\n",
    "                                                                          np.mean(intercept)))\n",
    "print(\"\\n\")\n",
    "# lets discuss monte carlo methods a little\n",
    "for i in range(20):\n",
    "    slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "    print(\"slope: {:+7.4f} $\\pm$ {:6.4f}, mean intercept: {:6.4f}\".format(np.mean(slope), np.std(slope), \n",
    "                                                                          np.mean(intercept)))\n",
    "\n",
    "# so what we see that our answer changes slightly, but its within the standard deviation\n",
    "# of each set of measurements. So we can quote a reliable answer, with a reliable uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'], data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'], data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "slope, intercept = mcFit(xdat, ydat, ydat_err)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'], linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_err), label='errors')\n",
    "ax.plot(data['X'], linear(data['X'], np.mean(slope), np.mean(intercept)), label='MC')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "ax.text(10, 0.4, \"Slope: {:+5.3f} $\\pm$ {:5.3f}\".format(np.mean(slope), np.std(slope)), fontsize=18)\n",
    "ax.text(10, 0.35, \"Slope: {:5.3f} $\\pm$ {:5.3f}\".format(np.mean(intercept), np.std(intercept)), fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here's an example of the use of lampda functions: simplifying or modifying another function\n",
    "params, other = opt.curve_fit(lambda x, b: linear(x, np.mean(slope), b), xdat, ydat)\n",
    "\n",
    "fixed_int = params[0]\n",
    "\n",
    "print(fixed_int)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data['X'], data['Y'],s=20,c='black',zorder=2)\n",
    "ax.errorbar(data['X'], data['Y'],yerr=data['Y_ERR'], c='tab:gray', \n",
    "            fmt='.', markersize=1, capsize=3 ,zorder=0)\n",
    "\n",
    "# along with out fits\n",
    "ax.plot(data['X'], linear(data['X'], *popt), label='raw')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_clean), label='better')\n",
    "ax.plot(data['X'], linear(data['X'], *popt_err), label='errors')\n",
    "ax.plot(data['X'], linear(data['X'], np.mean(slope), fixed_int), label='MC')\n",
    "\n",
    "ax.set_xlabel('$X$', fontsize=18)\n",
    "ax.set_ylabel('$Y$', fontsize=18)\n",
    "\n",
    "ax.text(10, 0.4, \"Slope: {:+5.3f} $\\pm$ {:5.3f}\".format(np.mean(slope), np.std(slope)), fontsize=18)\n",
    "# ax.text(10, 0.35, \"Slope: {:5.3f} $\\pm$ {:5.3f}\".format(np.mean(intercept), np.std(intercept)), fontsize=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise: we can't know the x data perfectly\n",
    "# suppose the data have uniform x errors of 0.5\n",
    "# use the MC technique to estimate the slope *AND* the uncertainty taking into account these errors\n",
    "# repeat this exercise, but supposing the errors increase as we move away from R_GC = 7 (as they do)\n",
    "# to be precise: suppose R_GC_ERR = |r_gc-7|/10, e.g. it increases by .1 per kpc (the units of R_GC)\n",
    "\n",
    "# plot both slopes. how does the fit change? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
